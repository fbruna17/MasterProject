{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install darts==0.16.1\nfrom typing import Tuple, Sequence, Optional, Union, Dict\nimport math\nfrom abc import ABC\nimport torch\nimport torch.nn.functional as F\nfrom torch import nn\nfrom torch.nn.utils import weight_norm\nfrom torch import Tensor\nfrom darts import TimeSeries\nfrom darts.logging import raise_if_not, get_logger\nfrom darts.utils.likelihood_models import Likelihood\nfrom darts.utils.torch import random_method\nfrom darts.utils.likelihood_models import QuantileRegression\nfrom darts.dataprocessing.transformers import Scaler\nfrom darts.models import TFTModel\nimport numpy as np\nfrom numpy.random.mtrand import RandomState\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nfrom darts_utility import SolarFlare, SolarFlare_No_Attention, SolarFlare_No_Bigru\n\nlogger = get_logger(__name__)","metadata":{"_uuid":"7e12b821-1dcb-4b01-adcd-66e524cdffd0","_cell_guid":"d001ffe8-915b-4b31-b84d-cea54ce237c5","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"path = '../input/irradiance-dataset/irradiance_dataset.csv'\ndf = pd.read_csv(path)[:60000]\ndf = df.drop(columns=\"Unnamed: 0\")\nmemory = 24*6\nhorizon = 5\nbatch = 256\n\ndf.insert(0, 'GHI', df.pop('GHI'))\n\n# Split data\ntarget_ts = TimeSeries.from_series(df[\"GHI\"]).astype(np.float32)\ntarget_train, target_val = target_ts.split_after(0.8)\ntarget_val, target_test = target_val.split_after(0.5)\n\n\npast_covar_ts = TimeSeries.from_dataframe(df[df.columns.to_list()[1:]]).astype(np.float32)\npast_covar_train, past_covar_val = past_covar_ts.split_after(0.8)\npast_covar_val, past_covar_test = past_covar_val.split_after(0.5)\n\n# Scale data\ntarget_ts_scaler = Scaler()\ntarget_train = target_ts_scaler.fit_transform(target_train)\ntarget_val = target_ts_scaler.transform(target_val)\ntarget_test = target_ts_scaler.transform(target_test)\nseries_transformed = target_ts_scaler.transform(target_ts)\n\ncovar_ts_scaler = Scaler()\ncovar_train = covar_ts_scaler.fit_transform(past_covar_train)\ncovar_val = covar_ts_scaler.transform(past_covar_val)\ncovar_test = covar_ts_scaler.transform(past_covar_test)\n","metadata":{"execution":{"iopub.status.busy":"2022-05-04T07:53:20.17942Z","iopub.execute_input":"2022-05-04T07:53:20.17968Z","iopub.status.idle":"2022-05-04T07:53:21.570811Z","shell.execute_reply.started":"2022-05-04T07:53:20.179644Z","shell.execute_reply":"2022-05-04T07:53:21.570097Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model1 = SolarFlare(input_chunk_length=memory,\n                   kernel_size=2,\n                   dilation_base=2,\n                   output_chunk_length=horizon,\n                   likelihood=QuantileRegression(quantiles=[0.01, 0.05, 0.2, 0.5, 0.8, 0.95, 0.99]),\n                   batch_size=batch,\n                   hidden_size=128,\n                   nr_epochs_val_period=1,\n                   log_tensorboard=False,\n                   torch_device_str= \"cuda:0\")\n\nmodel2 = SolarFlare_No_Attention(input_chunk_length=memory,\n                   kernel_size=2,\n                   dilation_base=2,\n                   output_chunk_length=horizon,\n                   likelihood=QuantileRegression(quantiles=[0.01, 0.05, 0.2, 0.5, 0.8, 0.95, 0.99]),\n                   batch_size=batch,\n                   hidden_size=128,\n                   nr_epochs_val_period=1,\n                   log_tensorboard=False,\n                   torch_device_str= \"cuda:0\")\n\nmodel3 = SolarFlare_No_Bigru(input_chunk_length=memory,\n                   kernel_size=2,\n                   dilation_base=2,\n                   output_chunk_length=horizon,\n                   likelihood=QuantileRegression(quantiles=[0.01, 0.05, 0.2, 0.5, 0.8, 0.95, 0.99]),\n                   batch_size=batch,\n                   hidden_size=128,\n                   nr_epochs_val_period=1,\n                   log_tensorboard=False,\n                   torch_device_str= \"cuda:0\")\n\nmodel4 = TFTModel(input_chunk_length=memory,\n                output_chunk_length=horizon,\n                hidden_size=128,\n                lstm_layers=1,\n                num_attention_heads=4,\n                dropout=0.2,\n                likelihood=QuantileRegression(quantiles=[0.01, 0.05, 0.2, 0.5, 0.8, 0.95, 0.99]),\n                batch_size=batch,\n                nr_epochs_val_period=1,\n                torch_device_str=\"cuda:0\")","metadata":{"execution":{"iopub.status.busy":"2022-05-04T07:53:21.572066Z","iopub.execute_input":"2022-05-04T07:53:21.572303Z","iopub.status.idle":"2022-05-04T07:53:21.659213Z","shell.execute_reply.started":"2022-05-04T07:53:21.572271Z","shell.execute_reply":"2022-05-04T07:53:21.658542Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model1.fit(series=target_train, past_covariates=covar_train, val_series=target_val, val_past_covariates=covar_val, verbose=True, num_loader_workers=2)\nmodel1.save_model(\"SolarFlare60k_bigru_attention.pth.tar\")\nplt.figure(figsize=(30, 7))\nbackcast1 = model1.historical_forecasts(series=target_train, past_covariates=past_covar_train, num_samples=600, start=0.98, retrain=False, verbose=True)\nbackcast1.quantile_df(quantile=0.5).to_csv(\"backcast1_60k_bigru_attention_median.csv\")\nseries_transformed.slice_intersect(backcast1).plot(label=\"target\")\nbackcast1.backcast.plot(label=\"forecast\", low_quantile=0.01, high_quantile=0.99)\nplt.xlabel(\"timesteps\")\nplt.ylabel(\"GHI\")\nplt.title(\"Backcast of model with attention and bigru\")\nplt.legend()\nplt.save(\"backcast1_60k_bigru_attention.png\")\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-05-04T07:53:21.661Z","iopub.execute_input":"2022-05-04T07:53:21.661245Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model2.fit(series=target_train, past_covariates=covar_train, val_series=target_val, val_past_covariates=covar_val, verbose=True, num_loader_workers=2)\nmodel2.save_model(\"SolarFlare30k_no_attention.pth.tar\")\nplt.figure(figsize=(30, 7))\nbackcast2 = model2.historical_forecasts(series=target_train, past_covariates=past_covar_train, num_samples=300, start=0.97, retrain=False, verbose=True)\nbackcast2.quantile_df(quantile=0.5).to_csv(\"backcast2_30k_no_attention.csv\")\nseries_transformed.slice_intersect(backcast2).plot(label=\"target\")\nbackcast2.plot(label=\"forecast\", low_quantile=0.01, high_quantile=0.99)\nplt.xlabel(\"timesteps\")\nplt.ylabel(\"GHI\")\nplt.title(\"Backcast of model with no attention\")\nplt.legend()\nplt.savefig(\"backcast2_30k_no_attention.svg\", format=\"svg\")\nplt.show()\n","metadata":{"execution":{"iopub.status.busy":"2022-05-03T12:11:07.652415Z","iopub.execute_input":"2022-05-03T12:11:07.652667Z","iopub.status.idle":"2022-05-03T13:17:23.536233Z","shell.execute_reply.started":"2022-05-03T12:11:07.652639Z","shell.execute_reply":"2022-05-03T13:17:23.535549Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model3.fit(series=target_train, past_covariates=covar_train, val_series=target_val, val_past_covariates=covar_val, verbose=True, num_loader_workers=2)\nmodel3.save_model(\"SolarFlare30k_no_bigru.pth.tar\")\nbackcast3 = model3.historical_forecasts(series=target_train, past_covariates=past_covar_train, num_samples=300, start=0.97, retrain=False, verbose=True)\nbackcast3.quantile_df(quantile=0.5).to_csv(\"backcast3_30k_no_bigru.csv\")\nplt.figure(figsize=(20, 20))\nseries_transformed.slice_intersect(backcast3).plot(label=\"target\")\nbackcast3.backcast.plot(label=\"forecast\", low_quantile=0.01, high_quantile=0.99)\nplt.xlabel(\"timesteps\")\nplt.ylabel(\"GHI\")\nplt.title(\"Backcast of model with no bidirectional gru\")\nplt.legend()\nplt.savefig(\"backcast3_30k_no_bigru.svg\", format=\"svg\")\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-05-03T10:36:57.980641Z","iopub.status.idle":"2022-05-03T10:36:57.981512Z","shell.execute_reply.started":"2022-05-03T10:36:57.981243Z","shell.execute_reply":"2022-05-03T10:36:57.981272Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(20, 7))\nseries_transformed.slice_intersect(backcast1).plot(label=\"target\")\nbackcast1.plot(label=\"forecast\", low_quantile=0.01, high_quantile=0.99)\nplt.xlabel(\"timesteps\")\nplt.ylabel(\"GHI\")\nplt.title(\"Backcast of model with attention and bigru\")\nplt.legend()\nplt.savefig(\"backcast1_60k_bigru_attention.svg\", format=\"svg\")\nplt.show()\n","metadata":{"execution":{"iopub.status.busy":"2022-05-03T11:58:17.645408Z","iopub.execute_input":"2022-05-03T11:58:17.64594Z","iopub.status.idle":"2022-05-03T11:58:18.755186Z","shell.execute_reply.started":"2022-05-03T11:58:17.645901Z","shell.execute_reply":"2022-05-03T11:58:18.754385Z"},"trusted":true},"execution_count":null,"outputs":[]}]}